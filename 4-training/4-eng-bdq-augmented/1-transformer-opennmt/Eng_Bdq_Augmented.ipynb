{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XsvDNvEzHQ4",
        "outputId": "4381b2e4-de1c-4d56-bf6e-7e36cc2e1853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WrtjJ7fvzjeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ee3ec4-9cb6-4a01-c877-fd7a0e1f199b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bahnar-English-Machine-Translation/4-training/4-eng-bdq-augmented/1-transformer-opennmt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Bahnar-English-Machine-Translation/4-training/4-eng-bdq-augmented/1-transformer-opennmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzXdIA3KAhlb"
      },
      "source": [
        "### Train a Subwording Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhX9ArmcE7e8",
        "outputId": "4ac5e327-0cc2-410b-d4ff-12d2b79fb85d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J71TLea4E_kX"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G18aGoGPFCzG"
      },
      "outputs": [],
      "source": [
        "train_source_file = \"/content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.train.eng\"\n",
        "train_target_file = \"/content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.train.bdq\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_train_value = '--input='+train_source_file+' --model_prefix=2-subwording-model/eng-source --vocab_size=32000 --hard_vocab_limit=false --model_type=bpe --split_digits=true'\n",
        "spm.SentencePieceTrainer.train(source_train_value)\n",
        "print(\"Done, training a SentencepPiece model for the Source finished successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiqRwfDLWSfe",
        "outputId": "29f7ffe0-9cd7-4bcd-961a-923464ae3a33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, training a SentencepPiece model for the Source finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVsA_TMAEKSM",
        "outputId": "3efdb935-b0c4-4a5e-ec33-e2a5f01731a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: 2-subwording-model/eng-source.model\n",
            "Dataset: /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.train.eng\n",
            "Done subwording the file! Output: 1-subworded-data/bdq-eng.train.subword.eng\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Bahnar-English-Machine-Translation/2-data-processing/subwording/2-subword.py 2-subwording-model/eng-source.model /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.train.eng 1-subworded-data/bdq-vie-eng.train.subword.eng"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Bahnar-English-Machine-Translation/2-data-processing/subwording/2-subword.py 2-subwording-model/eng-source.model /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.val.eng 1-subworded-data/bdq-vie-eng.val.subword.eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lJBoLkblK7O",
        "outputId": "69dbfdeb-e099-46ee-8d40-b60ebd86329f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: 2-subwording-model/eng-source.model\n",
            "Dataset: /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.val.eng\n",
            "Done subwording the file! Output: 1-subworded-data/bdq-vie-eng.val.subword.eng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME7BFhowqhAK",
        "outputId": "78a94a2d-3b6e-4c15-81d0-4babbbde1fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ],
      "source": [
        "target_train_value = '--input='+train_target_file+' --model_prefix=2-subwording-model/bdq-target --vocab_size=14000 --hard_vocab_limit=false --model_type=bpe --split_digits=true'\n",
        "spm.SentencePieceTrainer.train(target_train_value)\n",
        "print(\"Done, training a SentencepPiece model for the Target finished successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Bahnar-English-Machine-Translation/2-data-processing/subwording/2-subword.py 2-subwording-model/bdq-target.model /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.train.bdq 1-subworded-data/bdq-vie-eng.train.subword.bdq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5a0Hauyi2i_",
        "outputId": "ec4e9114-145a-40f3-ea42-b995193e2695"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: 2-subwording-model/bdq-target.model\n",
            "Dataset: /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.train.bdq\n",
            "Done subwording the file! Output: 1-subworded-data/bdq-vie-eng.train.subword.bdq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Bahnar-English-Machine-Translation/2-data-processing/subwording/2-subword.py 2-subwording-model/bdq-target.model /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.val.bdq 1-subworded-data/bdq-vie-eng.val.subword.bdq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kooLiHvsjW60",
        "outputId": "bba1de9c-adae-4da1-d49b-593b584c3b26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: 2-subwording-model/bdq-target.model\n",
            "Dataset: /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/1-train/2-with_aug/train-val-split/bdq-vie-eng.val.bdq\n",
            "Done subwording the file! Output: 1-subworded-data/bdq-vie-eng.val.subword.bdq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NSosw1LE_A4",
        "outputId": "7d5d7fd3-cb1b-425a-842e-0f58326c09be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁tastes ,\n",
            "▁Shall ▁horses ▁run ▁upon ▁the ▁rock\n",
            "▁Scope , ▁scale ▁and ▁duration ▁of ▁implementation\n",
            "▁ + ▁On ▁the ▁third ▁day : ▁After ▁the ▁pork ▁is ▁finished , ▁before ▁eating ▁and ▁drinking , ▁the ▁family ▁brings ▁the ▁shares ▁into ▁the ▁new ▁grave ▁house ▁for ▁the ▁deceased ▁and ▁weeps ▁for ▁the ▁last ▁time ▁goodbye ▁to ▁the ▁deceased ▁relatives , ▁while ▁the ▁host ▁reads ▁the ▁prayers ▁and ▁rituals ▁and ▁walks ▁around ▁the ▁grave ▁house ▁dancing , ▁playing ▁drums , ▁gongs , ▁t amb our ines ▁and ▁cymbals ▁until ▁the ▁fire ▁is ▁completely ▁extinguished ▁and ▁everyone ▁leaves .\n",
            "▁but ▁it ▁shall ▁be ▁one ▁day ▁which ▁shall ▁be ▁known ▁to ▁the ▁LORD , ▁not ▁day , ▁nor ▁night : ▁but ▁it ▁shall ▁come ▁to ▁pass , ▁that ▁at ▁evening ▁time ▁it ▁shall ▁be ▁light\n",
            "▁For ▁in ▁the ▁days ▁of ▁David ▁and ▁Asaph ▁of ▁old ▁there ▁were ▁chief ▁of ▁the ▁singers , ▁and ▁songs ▁of ▁praise ▁and ▁thanksgiving ▁unto ▁God\n",
            "▁young ▁out ,\n",
            "▁Then ▁said ▁Samuel ▁to ▁the ▁people , ▁Come , ▁and ▁let ▁us ▁go ▁to ▁Gilgal , ▁and ▁renew ▁the ▁kingdom ▁there\n",
            "▁All ▁the ▁Nethinims , ▁and ▁the ▁children ▁of ▁Solomon ' s ▁servants , ▁were ▁three ▁hundred ▁ninety ▁and ▁two\n",
            "▁All ▁the ▁Levites ▁in ▁the ▁holy ▁city ▁were ▁two ▁hundred ▁fourscore ▁and ▁four\n"
          ]
        }
      ],
      "source": [
        "!head -n 10 1-subworded-data/bdq-vie-eng.val.subword.eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bJ7eQbGktC-",
        "outputId": "fd705108-dda1-425e-c296-f9e99a5e281a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁mơ̆u\n",
            "▁Iĕm ▁hăm ▁lah ▁ăn ▁kơ ▁aseh ▁kơdâu ▁kơpal ▁tơmo ▁tih\n",
            "▁Pham ▁vi , ▁kuy ▁mô ▁weng ▁jơnang ▁pơm\n",
            "▁ + ▁‘ Năr ▁mă ▁pêng : ▁Đơ̆ng ▁rŏng ▁kơ ▁ƀuh ▁nhŭng , ▁adrol ▁kơ ▁et ▁xa , ▁unh ▁hnam ▁‘ nhăk ▁axong ▁tơmam ▁‘ nao ▁truh ▁pơxat ▁ăn ▁bơngai ▁lôch ▁păng ▁hmoi ▁‘ măng ▁mă ▁hơtuch ▁vă ▁tơklăh ▁hlŏng ▁hloi ▁hăm ▁bơngai ▁hiong , ▁lơ̆m ▁kơplăh ▁noh ▁tơm ▁hnam ▁tơbang ▁nơ̆r ▁xoi ▁atŭm ▁hăm ▁khôi ▁rok ▁tăp ▁dăr ▁pơxat ▁atŭm ▁hơ xuang , ▁tôm ▁chêng , ▁nŏ ▁nam ▁păng ▁rang ▁raih ▁truh ▁lai ▁yơ ▁jơmŭl ▁unh ▁păt ▁đĭ ▁noh ▁rim ▁bơngai ▁pơtơm ▁vih .\n",
            "▁Noh ▁jing ▁minh ▁năr ▁Kră ▁Yang ▁đĕch ▁băt ; ▁ưh ▁kơ ▁sĭ ▁kơ ' năr ▁dah ▁kơmăng , ▁mă - lei ▁dah ▁kơsơ̆ ▁truh ▁boih ▁gô ▁đei ▁hơdah\n",
            "▁Yua ▁kơ ▁lăm ▁chăl ▁Đawit ▁păng ▁Asap ▁sơ̆ , ▁đei ▁minh ▁' nu ▁kră ▁pơgơ̆r ▁wei - lăng ▁lu ▁bơngai ▁hơri , ▁đei ▁' bai ▁hơri ▁bơnê ▁kơkuh ▁ư - ang ▁kơ ▁Kră ▁Yang ▁' Bok ▁Kei - Dei\n",
            "▁' lơ̆p ▁loi ▁adrol\n",
            "▁Samuel ▁khan ▁kơ ▁kon ▁pơlei ▁pơla ▁thoi ▁âu : ▁Bĕ ▁bơ̆n ▁năm ▁tơ ▁Gilgal ▁wă ▁hơmet ▁ming ▁pơk ▁Sôl ▁pơm ▁pơtao\n",
            "▁Akŭm ▁pơđĭ ▁lu ▁đĭch ▁đam ▁kră ▁hnam ▁yang ▁păng ▁kon ▁sâu ▁lu ▁đĭch ▁đam ▁Sôlômôn , ▁pêng - hrĕng ▁tơsĭn - jĭt ▁' bar ▁' nu\n",
            "▁Bơngai ▁Lêwi ▁oei ▁lăm ▁pơlei ▁' lơ̆ng ▁rơgoh , ▁đei ▁' bar - hrĕng ▁tơhngam - jĭt ▁puăn ▁' nu\n"
          ]
        }
      ],
      "source": [
        "!head -n 10 1-subworded-data/bdq-vie-eng.val.subword.bdq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBDQy6vCJP4q"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMiM14CNJI9H"
      },
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZqWCc0YJRTu",
        "outputId": "02095a6d-7804-44d7-e956-749a14b688e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.4.3-py3-none-any.whl (257 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/257.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m256.0/257.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.2,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.1.0+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.17 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.60.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2.1.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=2.0.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=2.0.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.4.3 colorama-0.4.6 configargparse-1.7 ctranslate2-3.24.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.6.1 sacrebleu-2.4.0 waitress-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fjdeFaISI5KJ"
      },
      "outputs": [],
      "source": [
        "# Create the YAML configuration file\n",
        "# On a regular machine, you can create it manually or with nano\n",
        "# Note here we are using some smaller values because the dataset is small\n",
        "# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n",
        "\n",
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: vocabulary\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: 1-subworded-data/bdq-vie-eng.train.subword.eng\n",
        "        path_tgt: 1-subworded-data/bdq-vie-eng.train.subword.bdq\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: 1-subworded-data/bdq-vie-eng.val.subword.eng\n",
        "        path_tgt: 1-subworded-data/bdq-vie-eng.val.subword.bdq\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: vocabulary/eng-source.vocab\n",
        "tgt_vocab: vocabulary/bdq-target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 32000\n",
        "tgt_vocab_size: 14000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: 2-subwording-model/eng-source.model\n",
        "tgt_subword_model: 2-subwording-model/bdq-target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model:  4-model-checkpoints/enviba\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrwYg19WKGOE",
        "outputId": "4168ca23-dd6e-4aba-ce95-27174f2aef94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# config.yaml\n",
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: vocabulary\n",
            "\n",
            "# Training files\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: 1-subworded-data/bdq-vie-eng.train.subword.eng\n",
            "        path_tgt: 1-subworded-data/bdq-vie-eng.train.subword.bdq\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: 1-subworded-data/bdq-vie-eng.val.subword.eng\n",
            "        path_tgt: 1-subworded-data/bdq-vie-eng.val.subword.bdq\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabulary files, generated by onmt_build_vocab\n",
            "src_vocab: vocabulary/eng-source.vocab\n",
            "tgt_vocab: vocabulary/bdq-target.vocab\n",
            "\n",
            "# Vocabulary size - should be the same as in sentence piece\n",
            "src_vocab_size: 32000\n",
            "tgt_vocab_size: 14000\n",
            "\n",
            "# Filter out source/target longer than n if [filtertoolong] enabled\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenization options\n",
            "src_subword_model: 2-subwording-model/eng-source.model\n",
            "tgt_subword_model: 2-subwording-model/bdq-target.model\n",
            "\n",
            "# Where to save the log file and the output models/checkpoints\n",
            "log_file: train.log\n",
            "save_model:  4-model-checkpoints/enviba\n",
            "\n",
            "# Stop training if it does not imporve after n validations\n",
            "early_stopping: 4\n",
            "\n",
            "# Default: 5000 - Save a model checkpoint for each n\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# To save space, limit checkpoints to last n\n",
            "# keep_checkpoint: 3\n",
            "\n",
            "seed: 3435\n",
            "\n",
            "# Default: 100000 - Train the model to max n steps\n",
            "# Increase to 200000 or more for large datasets\n",
            "# For fine-tuning, add up the required steps to the original steps\n",
            "train_steps: 3000\n",
            "\n",
            "# Default: 10000 - Run validation after n steps\n",
            "valid_steps: 1000\n",
            "\n",
            "# Default: 4000 - for large datasets, try up to 8000\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Number of GPUs, and IDs of GPUs\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Optimization\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Model\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ],
      "source": [
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pbml6meKRu8"
      },
      "source": [
        "## Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8MnN-vpKJX3",
        "outputId": "4ac4f5ac-9249-4f5a-98ef-5d5a33c14f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "!nproc --all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvHTKYRBKU6m",
        "outputId": "38526104-42bf-49bf-f8fc-52937d117bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-02-08 11:05:40,507 INFO] Counter vocab from -1 samples.\n",
            "[2024-02-08 11:05:40,507 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-02-08 11:05:41,849 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=7)\n",
            "\n",
            "[2024-02-08 11:05:41,866 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=6)\n",
            "\n",
            "[2024-02-08 11:05:41,912 INFO] Counters src: 19447\n",
            "[2024-02-08 11:05:41,912 INFO] Counters tgt: 12250\n"
          ]
        }
      ],
      "source": [
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoVIURQOLAnp",
        "outputId": "68c83f57-38ca-4aee-d615-3b289f28a079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-90790112-1a23-1484-0cfd-16d5407ef500)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KU7JotMKed4",
        "outputId": "0d0f8b8c-e006-4c42-ef6d-4cd2224ee027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14999.0625 out of: 15102.0625\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlcEIP4lLXP0"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1wk3A4TLacd",
        "outputId": "e1758821-3c28-4765-c22a-683220f54f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-02-08 11:05:51,876 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-02-08 11:05:51,878 INFO] Parsed 2 corpora from -data.\n",
            "[2024-02-08 11:05:51,878 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-02-08 11:05:51,952 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', ',', '▁of', '▁and', '▁to', '▁in']\n",
            "[2024-02-08 11:05:51,952 INFO] The decoder start token is: <s>\n",
            "[2024-02-08 11:05:51,952 INFO] Building model...\n",
            "[2024-02-08 11:05:52,777 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-02-08 11:05:52,778 INFO] Non quantized layer compute is fp16\n",
            "[2024-02-08 11:05:53,084 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(19456, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(12256, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=12256, bias=True)\n",
            ")\n",
            "[2024-02-08 11:05:53,087 INFO] encoder: 28849152\n",
            "[2024-02-08 11:05:53,087 INFO] decoder: 37747680\n",
            "[2024-02-08 11:05:53,087 INFO] * number of parameters: 66596832\n",
            "[2024-02-08 11:05:53,088 INFO] Trainable parameters = {'torch.float32': 66596832, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-02-08 11:05:53,089 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-02-08 11:05:53,089 INFO]  * src vocab size = 19456\n",
            "[2024-02-08 11:05:53,089 INFO]  * tgt vocab size = 12256\n",
            "[2024-02-08 11:05:53,446 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-02-08 11:05:53,446 INFO] Starting training on GPU: [0]\n",
            "[2024-02-08 11:05:53,446 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-02-08 11:05:53,446 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/train.py\", line 67, in main\n",
            "    train(opt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/train.py\", line 52, in train\n",
            "    train_process(opt, device_id=0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/train_single.py\", line 237, in main\n",
            "    trainer.train(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/trainer.py\", line 308, in train\n",
            "    for i, (batches, normalization) in enumerate(self._accum_batches(train_iter)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/trainer.py\", line 238, in _accum_batches\n",
            "    for batch, bucket_idx in iterator:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/dynamic_iterator.py\", line 337, in __iter__\n",
            "    for bucket, bucket_idx in self._bucketing():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/dynamic_iterator.py\", line 271, in _bucketing\n",
            "    for ex in self.mixer:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/dynamic_iterator.py\", line 85, in __iter__\n",
            "    item = next(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 217, in __iter__\n",
            "    yield from corpus\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 176, in _process\n",
            "    for i, example in enumerate(stream):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 103, in load\n",
            "    yield make_ex(sline.decode(\"utf-8\"), tline, align)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_corpus.py\", line 63, in make_ex\n",
            "    sline, sfeats = parse_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/inputters/text_utils.py\", line 16, in parse_features\n",
            "    for token in line.split(\" \"):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9X1d4orL0ko"
      },
      "source": [
        "# Translation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ctranslate2"
      ],
      "metadata": {
        "id": "aq8dg9SEr5Ye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8bd19e-e05b-4c5f-b16c-9766c591c020"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ctranslate2 in /usr/local/lib/python3.10/dist-packages (3.24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2) (1.23.5)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2) (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-opennmt-py-converter --model_path 4-model-checkpoints/enviba_step_3000.pt --output_dir 4-model-checkpoints/enviba_ctranslate2 --quantization int8"
      ],
      "metadata": {
        "id": "wneIXog1r_XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/bdq-mt/evaluation/translate.py  /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/2-test/bdq-eng.test.eng 5-model-prediction/bdq-eng.test.eng.translated.bdq 2-subwording-model/eng-source.model 2-subwording-model/bdq-target.model models/enviba_ctranslate2"
      ],
      "metadata": {
        "id": "oxgRc1Xdrrbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f862ce5a-bc23-4b71-f286-9101281145de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 5-model-prediction/bdq-eng.test.eng.translated.bdq"
      ],
      "metadata": {
        "id": "3tIka8F0t8Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14ea59a-8cf5-4665-e2da-1ef89dd50fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Khul ngo hnhei bri phong hô huyên jơh pơjao khoan ăn 06 tơp thê, 184 unh hnam oei rim thôn lơm tơring xa ngo hnhei jư vei adring pơting ksô jên pơm loi 02 ty 604 triêu đông.\n",
            "Sư iung,\n",
            "Gah tơring bơngai Kanaan lu sư năm tơ gah hơlĕch păng tơ gah bơmơ̆t, noh kon pơlei Hêtit, kon pơlei Amôrit, kon pơlei Hêwit păng kon pơlei Jêbus\n",
            "sơreh (kơtau)\n",
            "Kră Yang 'Bok Kei-Dei wei-lăng lu bơ̆n, wă kơ bơ̆n huay kơ đei tơdrong kiơ, Sư oei hơdai hăm bơ̆n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/2-test/bdq-eng.test.bdq"
      ],
      "metadata": {
        "id": "5v60Gdr_uJBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659c4dde-f8e1-4132-e4ca-61f210326d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Khul ngo hnhĕi bri phong hô hŭn jơh pơjao khoan ăn 06 tơp thê, 184 unh hnam uĕi rim thôn lơm tơring sa ngo hnhĕi jư vĕi adrĭng pơting ksô jên pơm loi 02 ty 604 triêu đông.\n",
            "'yung hral\n",
            "Đei wơh dơ̆ng năm tơ kon pơlei Kanaan gah hơlĕch păng gah bơmơ̆t, tơ kon pơlei Amôrit, Hêtit, Phêrêsit păng kon pơlei Jêbus, oei kơpal kông groi; păng năm truh tơ kon pơlei Hêwit, jê̆ jơ̆ng kông Hermôn, lăm tơring Mispa\n",
            "Kơ̆p kông nhân\n",
            "'Bok Kei-Dei wă kơ lu sư gô hơdai bơ̆n dang ei iŏk tơdrong hiôk 'lơ̆ng đêl Sư pơkă, wă khan tơdrong Yêsu Krist jur dŏng pơklaih bơ̆n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAltMCfWL1zq"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3z8gmYDN4Xu",
        "outputId": "93fe1631-67e7-4a7b-ac55-380d4d063ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0zJ6P_9iBXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41162e13-6c8a-4646-b7f3-4936ca8d8284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU:  31.24\n",
            "CHRF: 47.55\n",
            "TER: 67.57\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/drive/MyDrive/bdq-mt/evaluation/eval-metrics.py /content/drive/MyDrive/Bahnar-English-Machine-Translation/3-dataset/bdq-eng/2-test/bdq-eng.test.bdq 5-model-prediction/bdq-eng.test.eng.translated.bdq"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}